{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Computer Science versus Business Management Introductory Course Professors Reviews and Their Trends Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "William Ingold, Erik Kelemen, Ashish Manda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying and requests of pages\n",
    "import requests\n",
    "\n",
    "# Parsing and handling HTML elements\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Storage and manipulation of data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Used to check for the existance of files\n",
    "from os import path\n",
    "\n",
    "# Utilities\n",
    "from itertools import chain\n",
    "import collections\n",
    "import re\n",
    "\n",
    "# Database and data storage\n",
    "import csv\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "# Selenium lets us load pages more natively, and can interact with the page\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# For handling the time & dates for reviews\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Handling errors in try blocks\n",
    "import traceback\n",
    "\n",
    "# Graphs and Visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# NlTK Libraries\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "## Only need to be run once\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# SpaCy\n",
    "## If using conda, do conda install -c conda-forge spacy\n",
    "import spacy\n",
    "import string # for punctuation list\n",
    "\n",
    "\n",
    "# Determine if we should utilize data storage\n",
    "should_store_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage: Setup Databases to Hold Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Part 1: Generic Database Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup database presets\n",
    "db_filepath = './data/db/'\n",
    "bmgt_rmp_db_filepath = db_filepath + 'bmgt_rmp.db'\n",
    "cmsc_rmp_db_filepath = db_filepath + 'cmsc_rmp.db'\n",
    "\n",
    "bmgt_pt_db_filepath = db_filepath + 'bmgt_pt.db'\n",
    "cmsc_pt_db_filepath = db_filepath + 'cmsc_pt.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\"Create a connection to the provided database file.\n",
    "    \n",
    "    Args:\n",
    "        db_file: A string holding the filepath to a database.\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = None\n",
    "    \n",
    "    if should_store_data:\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_file)\n",
    "            return conn\n",
    "        except Error as e:\n",
    "            print(e)\n",
    "\n",
    "    return conn\n",
    "\n",
    "\n",
    "def execute_create_command(conn, sql_command, params=()):\n",
    "    \"\"\"Executes the provided sql_command on the provided database.\n",
    "    \n",
    "    Args:\n",
    "        conn: The connection object to the database.\n",
    "        sql_command: A string containing the SQL command.\n",
    "        params: A tuple of potential parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(sql_command, params)\n",
    "        \n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "def execute_insert_command(conn, table_name, column_list, params=()):\n",
    "    \"\"\"Executes the provided sql_command on the provided database.\n",
    "    \n",
    "    Args:\n",
    "        conn: The connection object to the database.\n",
    "        sql_command: A string containing the SQL command.\n",
    "        params: A tuple of potential parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Question mark for each value to be filled, don't want a trailing comma\n",
    "    question_marks = \"?,\" * (len(column_list) - 1)\n",
    "    question_marks = question_marks + \"?\"\n",
    "    \n",
    "    column_names = \",\".join(column_list)\n",
    "    \n",
    "    insert_sql = \"\"\"INSERT INTO {table_name} (\n",
    "                                {column_names}\n",
    "                           )\n",
    "                           VALUES({question_marks})\n",
    "                           \"\"\".format(table_name=table_name, \n",
    "                                      question_marks=question_marks,\n",
    "                                      column_names=column_names)\n",
    "    \n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(insert_sql, params)\n",
    "        conn.commit()\n",
    "        \n",
    "        return c.lastrowid\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def execute_query_command(conn, sql_command, params=()):\n",
    "    \"\"\"Executes the provided sql_command on the provided database.\n",
    "    \n",
    "    Args:\n",
    "        conn: The connection object to the database.\n",
    "        sql_command: A string containing the SQL command.\n",
    "        params: A tuple of potential parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(sql_command, params)\n",
    "        \n",
    "        return c.fetchall()\n",
    "    \n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "def is_professor_scraped(db_conn, professor_name):\n",
    "    \"\"\"Returns if the professor's RateMyProfessors page has been scraped already.\n",
    "    \n",
    "    Args:\n",
    "        db_conn: Connection object to the appropriate database.\n",
    "        professor_name: String holding the professor's name.\n",
    "    \"\"\"\n",
    "    \n",
    "    if should_store_data:\n",
    "        sql_command = \"\"\"SELECT\n",
    "                            full_name\n",
    "                        FROM\n",
    "                            professor_stats ps\n",
    "                        WHERE\n",
    "                            full_name LIKE ?\"\"\"\n",
    "\n",
    "        params=('%'+professor_name+'%',)\n",
    "\n",
    "        result = execute_query_command(db_conn, sql_command, params)\n",
    "\n",
    "        return len(result) != 0\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def insert_dataframe_into_db(db_conn, df, table_name, column_headers=None):\n",
    "    \"\"\"Inserts all rows of a given dataframe to the database's table.\n",
    "    \n",
    "    Args:\n",
    "        db_conn: Connection object to a database.\n",
    "        df: Pandas DataFrame object containing data to insert.\n",
    "        table_name: String holding a table name to insert into ('reviews' or 'professor_stats')\n",
    "    \"\"\"\n",
    "    \n",
    "    if should_store_data:\n",
    "        if column_headers is None:\n",
    "            column_list = list(df.columns)\n",
    "        else:\n",
    "            column_list = column_headers\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            execute_insert_command(db_conn, table_name, column_list, tuple(row.array))\n",
    "\n",
    "\n",
    "def get_professor_stats_from_db(db_conn, professor):\n",
    "    \"\"\"Reads the professor_stats table into a pandas dataframe and returns it.\"\"\"\n",
    "    \n",
    "    sql_query = \"\"\"SELECT * FROM professor_stats WHERE full_name LIKE ?\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, db_conn, params=[professor])\n",
    "\n",
    "def get_professor_reviews_from_db(db_conn, professor):\n",
    "    \"\"\"Reads the reviews table into a pandas dataframe and returns it.\"\"\"\n",
    "    \n",
    "    sql_query = \"\"\"SELECT * FROM reviews WHERE full_name LIKE ?\"\"\"\n",
    "    \n",
    "    return pd.read_sql_query(sql_query, db_conn, params=[professor])\n",
    "\n",
    "def get_course_grades_from_db(db_conn, course):\n",
    "    \"\"\"Reads the grades table for a certain course into a pandas data\n",
    "    frame and returns it.\"\"\"\n",
    "    \n",
    "    if should_store_data:\n",
    "        sql_query = \"\"\"SELECT * FROM grades WHERE course LIKE ?\"\"\"\n",
    "\n",
    "        return pd.read_sql_query(sql_query, db_conn, params=[course])\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Part 2: RateMyProfessor Specific Database Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rmp_tables(rmp_conn):\n",
    "    \"\"\"Create the stats and review tables for RateMyProfessors data.\n",
    "    \n",
    "    Args:\n",
    "        rmp_conn: Connection object to a RateMyProfessors database.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats_table = \"\"\" CREATE TABLE IF NOT EXISTS professor_stats (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        first_name TEXT NOT NULL,\n",
    "                        last_name TEXT NOT NULL,\n",
    "                        full_name TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        page_exists INTEGER NOT NULL,\n",
    "                        rating REAL,\n",
    "                        take_again REAL,\n",
    "                        difficulty REAL,\n",
    "                        rating_count INTEGER NOT NULL,\n",
    "                        gives_good_feedback INTEGER,\n",
    "                        respected INTEGER,\n",
    "                        lots_of_homework INTEGER,\n",
    "                        accessible_outside_class INTEGER,\n",
    "                        get_ready_to_read INTEGER,\n",
    "                        participation_matters INTEGER,\n",
    "                        skip_class_wont_pass INTEGER,\n",
    "                        inspirational INTEGER,\n",
    "                        graded_by_few_things INTEGER,\n",
    "                        test_heavy INTEGER,\n",
    "                        group_projects INTEGER,\n",
    "                        clear_grading_criteria INTEGER,\n",
    "                        hilarious INTEGER,\n",
    "                        beware_of_pop_quizes INTEGER,\n",
    "                        amazing_lectures INTEGER,\n",
    "                        lecture_heavy INTEGER,\n",
    "                        caring INTEGER,\n",
    "                        extra_credit INTEGER,\n",
    "                        so_many_papers INTEGER,\n",
    "                        tough_grader INTEGER\n",
    "                    ) \"\"\"\n",
    "    \n",
    "    # Review id format <professor last name>-<#> \n",
    "    review_table = \"\"\" CREATE TABLE IF NOT EXISTS reviews (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        review_id TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        first_name TEXT NOT NULL,\n",
    "                        last_name TEXT NOT NULL,\n",
    "                        full_name TEXT NOT NULL,\n",
    "                        course TEXT NOT NULL,\n",
    "                        date INTEGER NOT NULL,\n",
    "                        body TEXT NOT NULL,\n",
    "                        thumb_up INTEGER,\n",
    "                        thumb_down INTEGER,\n",
    "                        quality REAL NOT NULL,\n",
    "                        difficulty REAL NOT NULL,\n",
    "                        would_take_again INTEGER NOT NULL,\n",
    "                        for_credit INTEGER NOT NULL,\n",
    "                        textbook INTEGER NOT NULL,\n",
    "                        attendance INTEGER,\n",
    "                        grade TEXT,\n",
    "                        online_class INTEGER,\n",
    "                        gives_good_feedback INTEGER,\n",
    "                        respected INTEGER,\n",
    "                        lots_of_homework INTEGER,\n",
    "                        accessible_outside_class INTEGER,\n",
    "                        get_ready_to_read INTEGER,\n",
    "                        participation_matters INTEGER,\n",
    "                        skip_class_wont_pass INTEGER,\n",
    "                        inspirational INTEGER,\n",
    "                        graded_by_few_things INTEGER,\n",
    "                        test_heavy INTEGER,\n",
    "                        group_projects INTEGER,\n",
    "                        clear_grading_criteria INTEGER,\n",
    "                        hilarious INTEGER,\n",
    "                        beware_of_pop_quizes INTEGER,\n",
    "                        amazing_lectures INTEGER,\n",
    "                        lecture_heavy INTEGER,\n",
    "                        caring INTEGER,\n",
    "                        extra_credit INTEGER,\n",
    "                        so_many_papers INTEGER,\n",
    "                        tough_grader INTEGER\n",
    "                   ) \"\"\"\n",
    "    \n",
    "    execute_create_command(rmp_conn, stats_table)\n",
    "    execute_create_command(rmp_conn, review_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_store_data:\n",
    "    # Create the CMSC and BMGT database with the two tables\n",
    "    cmsc_rmp_db = create_connection(cmsc_rmp_db_filepath)\n",
    "    bmgt_rmp_db = create_connection(bmgt_rmp_db_filepath)\n",
    "\n",
    "    create_rmp_tables(cmsc_rmp_db)\n",
    "    create_rmp_tables(bmgt_rmp_db)\n",
    "\n",
    "    # Close for now, will reopen when writing to them\n",
    "    cmsc_rmp_db.close()\n",
    "    bmgt_rmp_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage Part 3: PlanetTerp Database Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pt_tables(pt_conn):\n",
    "    \"\"\"Create the stats and review tables for RateMyProfessors data.\n",
    "    \n",
    "    Args:\n",
    "        pt_conn: Connection object to a PlanetTerp database.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Keep track of grade distribution in this table?\n",
    "    stats_table = \"\"\" CREATE TABLE IF NOT EXISTS professor_stats (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        first_name TEXT NOT NULL,\n",
    "                        last_name TEXT NOT NULL,\n",
    "                        full_name TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        page_exists INTEGER NOT NULL,\n",
    "                        slug TEXT,\n",
    "                        review_count INTEGER NOT NULL,\n",
    "                        rating REAL,\n",
    "                        type TEXT\n",
    "                    ) \"\"\"\n",
    "    \n",
    "    # TODO: Review id format? <professor last name>-<#> ?\n",
    "    review_table = \"\"\" CREATE TABLE IF NOT EXISTS reviews (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        review_id TEXT NOT NULL UNIQUE ON CONFLICT IGNORE,\n",
    "                        full_name TEXT NOT NULL,\n",
    "                        course TEXT NOT NULL,\n",
    "                        date INTEGER NOT NULL,\n",
    "                        body TEXT NOT NULL,\n",
    "                        rating INTEGER NOT NULL,\n",
    "                        expected_grade TEXT\n",
    "                   ) \"\"\"\n",
    "    \n",
    "    grades_table = \"\"\" CREATE TABLE IF NOT EXISTS grades (\n",
    "                            id INTEGER PRIMARY KEY,\n",
    "                            course TEXT NOT NULL,\n",
    "                            semester INTEGER,\n",
    "                            a_plus INTEGER,\n",
    "                            a INTEGER,\n",
    "                            a_minus INTEGER,\n",
    "                            b_plus INTEGER,\n",
    "                            b INTEGER,\n",
    "                            b_minus INTEGER,\n",
    "                            c_plus INTEGER,\n",
    "                            c INTEGER,\n",
    "                            c_minus INTEGER,\n",
    "                            d_plus INTEGER,\n",
    "                            d INTEGER,\n",
    "                            d_minus INTEGER,\n",
    "                            f INTEGER,\n",
    "                            w INTEGER,\n",
    "                            UNIQUE(course, semester) ON CONFLICT IGNORE\n",
    "                    )\"\"\"\n",
    "    \n",
    "    execute_create_command(pt_conn, stats_table)\n",
    "    execute_create_command(pt_conn, review_table)\n",
    "    execute_create_command(pt_conn, grades_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_store_data:\n",
    "    # Create the CMSC and BMGT database with the two tables\n",
    "    cmsc_pt_db = create_connection(cmsc_pt_db_filepath)\n",
    "    bmgt_pt_db = create_connection(bmgt_pt_db_filepath)\n",
    "\n",
    "    create_pt_tables(cmsc_pt_db)\n",
    "    create_pt_tables(bmgt_pt_db)\n",
    "\n",
    "    # Close for now, will reopen when writing to them\n",
    "    cmsc_pt_db.close()\n",
    "    bmgt_pt_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO: Alternative database structure, where each professor has its own tables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_rmp_professor_overall_table(rmp_conn, professor_name, overall_df):\n",
    "    table_name = professor_name + \"_stats\"\n",
    "    overall_df.to_sql(table_name, con=rmp_conn, if_exists='append')\n",
    "    \n",
    "def insert_rmp_professor_reviews_table(rmp_conn, professor_name, review_df):\n",
    "    table_name = professor_name + \"_reviews\"\n",
    "    review_df.to_sql(table_name, con=rmp_conn, if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Accomplished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 1: Grabbing Introductory Course Professors From UMD.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base API url for UMD.io\n",
    "professors_url = \"https://api.umd.io/v1/professors\"\n",
    "\n",
    "# Base API url for PlanetTerp\n",
    "pt_courses_url = \"https://api.planetterp.com/v1/course\"\n",
    "\n",
    "# The filepaths for the files to hold professor information\n",
    "cmsc_professor_names_filepath = './data/cmsc_professor_names.csv'\n",
    "bmgt_professor_names_filepath = './data/bmgt_professor_names.csv'\n",
    "\n",
    "# Determines if we've created these already\n",
    "have_cmsc_professors = path.exists(cmsc_professor_names_filepath)\n",
    "have_bmgt_professors = path.exists(bmgt_professor_names_filepath)\n",
    "\n",
    "\n",
    "# Courses we're interseted in look at\n",
    "# TODO: What about honors?\n",
    "# TODO: Dr. Eastman has taught CMSC131, per RateMyProfessor, but wasn't given via UMD.IO\n",
    "cmsc_course_ids = [\"CMSC131\", \"CMSC132\", \"CMSC216\", \"CMSC250\"]\n",
    "bmgt_course_ids = [\"BMGT110\", \"BMGT220\", \"BMGT221\", \"BMGT230\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for saving professor data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_professor_name_data(professor_filepath):\n",
    "    \"\"\"Reads the professor names and their courses from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        professor_filepath: String holding a filepath to the professor csv file.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of professor names to a set of courses they have taught.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(professor_filepath, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        line_count = 0\n",
    "\n",
    "        professors = {}\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if line_count != 0:\n",
    "                professors[row['name']] = set([course for course in row['courses'].split(' ')])\n",
    "            line_count += 1\n",
    "\n",
    "        return professors\n",
    "\n",
    "def save_professor_data(professors, filepath):\n",
    "    \"\"\"Saves the professor names and their courses to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        professors: A dictionary of professor name keys and a set of courses for values.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['name', 'courses']\n",
    "    try:\n",
    "        with open(filepath, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for name, courses in professors.items():\n",
    "                writer.writerow({'name': name, 'courses': ' '.join(courses)})\n",
    "                \n",
    "    except IOError:\n",
    "        print(\"Error in writing the CSV file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility to actually grab professors based on a list of courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_professors_for_courses_from_umdio(course_ids):\n",
    "    \"\"\"Gets all the professors for the given course_ids from UMD.io \n",
    "    and returns a dictionary of professor to courses.\n",
    "    \n",
    "    Args:\n",
    "        course_ids: A list of course ids (e.g. ['CMSC216', CMSC250']).\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of professor to set of courses.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    professors = {}\n",
    "    \n",
    "    for course_id in course_ids:\n",
    "        params = {'course_id': course_id}\n",
    "\n",
    "        response = requests.get(professors_url, params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "\n",
    "            for item in response.json():\n",
    "                name = item['name']\n",
    "\n",
    "                if name in professors:\n",
    "                    professors[name].add(course_id)\n",
    "                else:\n",
    "                    professors[name] = {course_id}\n",
    "\n",
    "    return professors\n",
    "\n",
    "\n",
    "def get_professors_for_courses_from_pt(course_ids):\n",
    "    \"\"\"Gets all the professors for the given course_ids from PlanetTerp\n",
    "    and returns a dictionary of professor to courses.\n",
    "    \n",
    "    Args:\n",
    "        course_ids: A list of course ids (e.g. ['CMSC216', CMSC250']).\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of professor to set of courses.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    professor_list = {}\n",
    "    \n",
    "    for course_id in course_ids:\n",
    "        params = {'name': course_id}\n",
    "\n",
    "        response = requests.get(pt_courses_url, params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            professors = response.json().get('professors', None)\n",
    "            \n",
    "            if professors:\n",
    "                for professor in professors:\n",
    "                    if professor in professor_list:\n",
    "                        professor_list[professor].add(course_id)\n",
    "                    else:\n",
    "                        professor_list[professor] = {course_id}\n",
    "\n",
    "    return professor_list\n",
    "\n",
    "def combine_professor_dictionaries(dict_one, dict_two):\n",
    "    combined_profs = collections.defaultdict(set)\n",
    "\n",
    "    for key, val in chain(dict_one.items(), dict_two.items()):\n",
    "        combined_profs[key] = combined_profs[key].union(val)\n",
    "        \n",
    "    return combined_profs\n",
    "\n",
    "def get_all_professors_from_courses(course_ids):\n",
    "    umdio_professors = get_professors_for_courses_from_umdio(course_ids)\n",
    "    pt_professors = get_professors_for_courses_from_pt(course_ids)\n",
    "    \n",
    "    return combine_professor_dictionaries(umdio_professors, pt_professors)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Computer Science Professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only query the UMD.io API if we don't have the data\n",
    "if not have_cmsc_professors or not should_store_data:\n",
    "    cmsc_professors = get_all_professors_from_courses(cmsc_course_ids)\n",
    "    \n",
    "    save_professor_data(cmsc_professors, cmsc_professor_names_filepath)\n",
    "    have_cmsc_professors = True\n",
    "else: \n",
    "    cmsc_professors = read_professor_name_data(cmsc_professor_names_filepath)\n",
    "\n",
    "    if not cmsc_professors:\n",
    "        print(\"Error response from umd.io API\")\n",
    "\n",
    "if 'Iason Filippou' in cmsc_professors:\n",
    "    cmsc_professors.pop('Iason Filippou') # A typo of Jason Filippou from the database\n",
    "    \n",
    "print(cmsc_professors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Business Management Professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only query the UMD.io API if we don't have the data\n",
    "if not have_bmgt_professors or not should_store_data:\n",
    "    bmgt_professors = get_all_professors_from_courses(bmgt_course_ids)\n",
    "    \n",
    "    save_professor_data(bmgt_professors, bmgt_professor_names_filepath)\n",
    "    have_bmgt_professors = True\n",
    "else:\n",
    "    bmgt_professors = read_professor_name_data(bmgt_professor_names_filepath)\n",
    "\n",
    "    if not bmgt_professors:\n",
    "        print(\"Error response from umd.io API\")\n",
    "\n",
    "print(bmgt_professors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 2: Grabbing Reviews From RateMyProfessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.1: Setup and Utilities to Scrape and Parse Data from RateMyProfessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection Part 2.1.1: Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data needed for requesting data from RateMyProfessor\n",
    "ratemyprofessor_url = \"https://www.ratemyprofessors.com/search.jsp\"\n",
    "params = {'queryoption':'HEADER', 'schoolID':'1270', 'queryBy':'teacherName', 'schoolName':'University+of+Maryland'}\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:82.0) Gecko/20100101 Firefox/82.0\",\n",
    "    \"Access-Control-Allow-Origin\": \"*\",\n",
    "    \"Access-Control-Allow-Headers\": \"Content-Type\",\n",
    "    \"Access-Control-Allow-Methods\": \"GET\",\n",
    "}\n",
    "\n",
    "\n",
    "# List of tags that RateMyProfessor uses to describe professors, which are used for the database and dataframes\n",
    "tag_list = ['gives_good_feedback', 'respected', 'lots_of_homework', 'accessible_outside_class',\n",
    "           'get_ready_to_read', 'participation_matters', 'inspirational',\n",
    "           'graded_by_few_things', 'test_heavy', 'group_projects', 'clear_grading_criteria', \n",
    "           'hilarious', 'beware_of_pop_quizes', 'amazing_lectures', 'lecture_heavy', 'caring',\n",
    "           'extra_credit', 'so_many_papers', 'tough_grader', 'skip_class_wont_pass']\n",
    "\n",
    "# Want to tie the code friendly tag names to what is found on a RateMyProfessor page\n",
    "text_tag_list = [' '.join(x.split('_')) for x in tag_list]\n",
    "text_tag_list.remove('skip class wont pass')\n",
    "text_tag_list.append(\"skip class? you won't pass.\")\n",
    "\n",
    "# both tag_list and text_tag_list in same order, and correspond to one another\n",
    "text_tag_dict = {text_tag_list[i]: tag_list[i] for i in range(len(text_tag_list))}\n",
    "\n",
    "# These are the column headers for a professor's overall statistics found at the top of the page\n",
    "overall_header_list = ['first_name', 'last_name', 'full_name', 'page_exists', 'rating', 'take_again', 'difficulty',\n",
    "                      'rating_count'] + tag_list\n",
    "\n",
    "# Review post column headers. The meta list is the row of top meta responses (like 'Grade: A-').\n",
    "review_meta_list = ['would_take_again', 'grade', 'textbook', 'online_class', 'for_credit', 'attendance']\n",
    "review_text_meta_list = [' '.join(x.split('_')) for x in review_meta_list]\n",
    "review_meta_dict = {review_text_meta_list[i]: review_meta_list[i] for i in range(len(review_meta_list))}\n",
    "        \n",
    "review_header_list = ['review_id', 'course', 'date', 'quality', 'difficulty', 'body',\n",
    "                      'thumb_up', 'thumb_down'] + review_meta_list + tag_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection Part 2.1.2: Utility Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_dict(provided_tags):\n",
    "    \"\"\"Turns the list of text tags (e.g. skip class? you won't pass) into a dictionary\n",
    "    of approriately named tags that work for database columns and if they were present.\n",
    "    \n",
    "    Args:\n",
    "        provided_tags: A list of space separated tags scraped from the RMP page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of {tag: 1 or 0} on whether a tag was used to describe the professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_dict = {val: 0 for val in text_tag_dict.values()}\n",
    "    \n",
    "    for tag in provided_tags:\n",
    "        if tag.lower() in text_tag_dict.keys():\n",
    "            tag_dict[text_tag_dict[tag.lower()]] = 1\n",
    "            \n",
    "    return tag_dict\n",
    "\n",
    "def meta_to_dict(provided_meta):\n",
    "    \"\"\"Turns the dictionary of meta tags (e.g. Would Take Again: No) into a dictionary\n",
    "    of appropriately named tags that work for database columns and values if they were present.\n",
    "    \n",
    "    Args:\n",
    "        provided_meta: A dictionary of meta information from a review.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of {meta: 1 or 0} on whether a meta was used on the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    meta_dict = {val: 0 for val in review_meta_list}\n",
    "    \n",
    "    for meta, response in provided_meta.items():\n",
    "        value = 0\n",
    "        \n",
    "        if meta.lower() in review_meta_dict.keys():\n",
    "            \n",
    "            if response.lower() == \"yes\" or response.lower() == \"mandatory\":\n",
    "                value = 1\n",
    "                \n",
    "            if meta.lower() == \"grade\":\n",
    "                value = response\n",
    "            \n",
    "            meta_dict[review_meta_dict[meta.lower()]] = value\n",
    "            \n",
    "    return meta_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.2: Querying RateMyProfessor and Getting the Professor's URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rmp_professor_url(html_doc):\n",
    "    \"\"\"Finds the professor's URL on the search page and returns it.\n",
    "    \n",
    "    Args:\n",
    "        html_doc: A string containing an HTML document.\n",
    "        \n",
    "    Returns:\n",
    "        The full URL for the professor's page (if found).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    no_results = soup.find('div[class*=\"NoResultsFoundArea__StyledNoResultsFound\"]')\n",
    "    partial_url = soup.find('li', class_='listing PROFESSOR')\n",
    "    \n",
    "    # Sometimes RMP does the search differntly, so it'll be elsewhere\n",
    "    diff_location = soup.find('a', attrs={'class': lambda x: 'TeacherCard__StyledTeacherCard' in x if x else False}, href=True)\n",
    "    \n",
    "    # The professor may not be reviewed\n",
    "    if no_results is None and partial_url and len(partial_url) != 0:\n",
    "        if diff_location:\n",
    "            partial_url = diff_location['href']\n",
    "        else:\n",
    "            partial_url = partial_url.find('a', href=True)\n",
    "\n",
    "        if partial_url:\n",
    "            main_url = \"https://www.ratemyprofessors.com\"\n",
    "            return main_url + partial_url['href']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def query_rmp_for_professor_url(professor_name, headers, params):\n",
    "    \"\"\"Queries RateMyProfessor for the professor, given the parameters and headers.\n",
    "    \n",
    "    Args:\n",
    "        professor_name: The <first name> <last name> of the professor.\n",
    "        headers: Dictionary of headers for the get request.\n",
    "        params: Dictionary of parameters for the get request.\n",
    "        \n",
    "    Returns:\n",
    "        The full URL for the professor's page after searching for it (if found).\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    params['query'] = professor_name\n",
    "    \n",
    "    response = requests.get(ratemyprofessor_url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        url = find_rmp_professor_url(response.text)\n",
    "        \n",
    "        if url is not None:\n",
    "            return url\n",
    "        else:\n",
    "            print(\"Professor {name} has not been reviewed.\".format(name=professor_name))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.3: Parsing the Professor Overall Information (Stats and Tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmp_prof_stats(page_text):\n",
    "    \"\"\"Parses the professor's stats from their page and returns them. Namely their overall rating, \n",
    "    how many would take again, overall difficulty and how many ratings they have on RateMyProfessor.\n",
    "    \n",
    "    Args:\n",
    "        page_text: An HTML document of the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing their rating, take again percentage, difficulty rating, and rating count.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(page_text, 'html.parser')\n",
    "    \n",
    "    rating_score = soup.select('div[class*=\"RatingValue__Numerator\"]')\n",
    "    \n",
    "    take_again = np.nan\n",
    "    difficulty = np.nan\n",
    "    \n",
    "    if rating_score is not None and rating_score[0].text != 'N/A':\n",
    "        rating_score = float(rating_score[0].text)\n",
    "    else:\n",
    "        rating_score = np.nan\n",
    "    \n",
    "    feedback_cont = soup.select('div[class*=\"TeacherFeedback__StyledTeacherFeedback\"]') #[0].select('div[class*=\"FeedbackItem__FeedbackNumber\"]')\n",
    "    \n",
    "    if feedback_cont and len(feedback_cont) > 0:\n",
    "        feedback_nums = feedback_cont[0].select('div[class*=\"FeedbackItem__FeedbackNumber\"]')\n",
    "        \n",
    "        if feedback_nums and len(feedback_nums) == 2:\n",
    "            if len(feedback_nums[0].text) > 0:\n",
    "                try:\n",
    "                    take_again = float(feedback_nums[0].text[:-1]) / 100\n",
    "                except ValueError:\n",
    "                    take_again = np.nan\n",
    "                \n",
    "            if len(feedback_nums[1].text) > 0:\n",
    "                try:\n",
    "                    difficulty = float(feedback_nums[1].text)\n",
    "                except ValueError:\n",
    "                    difficulty = np.nan\n",
    "    \n",
    "    rating_count_int = 0\n",
    "    rating_count_cont = soup.select('div[class*=\"RatingValue__NumRatings\"]') #[0].select('a')[0].text\n",
    "    \n",
    "    if rating_count_cont and len(rating_count_cont) > 0:\n",
    "        rating_count_a = rating_count_cont[0].select('a')\n",
    "        \n",
    "        if rating_count_a and len(rating_count_a) > 0:\n",
    "            rating_count = rating_count_a[0].text\n",
    "            \n",
    "            try:\n",
    "                rating_count_int = int(rating_count)\n",
    "            except ValueError:\n",
    "                rating_count_int = 0\n",
    "            \n",
    "        \n",
    "    rating_count = ''.join([x for x in rating_count if x.isdigit()])\n",
    "    \n",
    "    return {'rating': rating_score, 'take_again': take_again, 'difficulty': difficulty, 'rating_count': rating_count_int}\n",
    "\n",
    "\n",
    "def get_rmp_prof_top_tags(page_text):\n",
    "    \"\"\"Parses and returns the professor's top tags.\n",
    "    \n",
    "    Args:\n",
    "        page_text: An HTML document of the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A list of tags describing the professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(page_text, 'html.parser')\n",
    "    \n",
    "    tags = []\n",
    "    unparsed_tags = soup.select('div[class*=\"TeacherTags__TagsContainer\"]')\n",
    "    \n",
    "    if unparsed_tags and len(unparsed_tags) != 0:\n",
    "        unparsed_tags = unparsed_tags[0].select('span')\n",
    "    \n",
    "        for tag in unparsed_tags:\n",
    "            tags.append(tag.text)\n",
    "        \n",
    "    return tags_to_dict(tags)\n",
    "\n",
    "\n",
    "def rmp_prof_overall_to_dataframe(professor_name, stats, tags, page_exists=1):\n",
    "    \"\"\"Combines the professor's overall stats and tags into a pandas dataframe.\n",
    "    \n",
    "    Args:\n",
    "        professor_name: String holding the professor's name.\n",
    "        stats: A dictionary holding the overall stats (e.g. 'would_take_again': .83).\n",
    "        tags: A dictionary holding the tags associated with a professor (e.g. {'caring': 1}).\n",
    "        page_exists (optional, default=1): Integer boolean determining if a professor has a RMP page.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe containing the combination of professor name, stats, and tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_df = pd.DataFrame(columns=overall_header_list)\n",
    "    \n",
    "    first_name, last_name = professor_name.split(' ', 1)\n",
    "    overall_dict = {'first_name': first_name, 'last_name': last_name, 'full_name': professor_name, 'page_exists': page_exists}\n",
    "    \n",
    "    overall_dict.update(stats)\n",
    "    overall_dict.update(tags)\n",
    "    \n",
    "    overall_df = overall_df.append(overall_dict, ignore_index=True)\n",
    "    \n",
    "    return overall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.4: Use Selenium to Load All Professor Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_selenium():\n",
    "    \"\"\"Starts up the Selenium browser.\"\"\"\n",
    "    driver = webdriver.Firefox(executable_path='./bin/geckodriver.exe')\n",
    "    return driver\n",
    "    \n",
    "def stop_selenium(driver):\n",
    "    \"\"\"Shutdown the Selenium browser.\"\"\"\n",
    "    driver.close()\n",
    "    driver.quit()\n",
    "    \n",
    "def load_all_rmp_reviews(page_url, driver):\n",
    "    \"\"\"Loads all the reviews for a given porfessor and returns the text of all of them.\n",
    "    \n",
    "    Args:\n",
    "        page_url: The URL for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the HTML for all the reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    driver.get(page_url)\n",
    "    \n",
    "    # RateMyProfessors has a cookies pop up that overlays the website, it needs to be closed first\n",
    "    time.sleep(0.5)\n",
    "    close_cookies = driver.find_elements(By.XPATH, '//button[text()=\"Close\"]')\n",
    "    \n",
    "    if close_cookies:\n",
    "        close_cookies[0].click()\n",
    "        \n",
    "    load_more = driver.find_elements(By.XPATH, '//button[text()=\"Load More Ratings\"]')\n",
    "    \n",
    "    # RateMyProfessors paginates the reviews via Javascript, so we must continually load more while the button is present\n",
    "    while load_more:\n",
    "        load_more[0].click()\n",
    "        time.sleep(1)\n",
    "        load_more = driver.find_elements(By.XPATH, '//button[text()=\"Load More Ratings\"]')\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        all_reviews = driver.find_element_by_id('ratingsList').get_attribute('outerHTML')\n",
    "    except NoSuchElementException:\n",
    "        all_reviews = ''\n",
    "    \n",
    "    \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.5: Parsing Utilities for a Single Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_date_to_unix(date_str):\n",
    "    \"\"\"Turns the RateMyProfessor date format (e.g. Nov 23rd, 2020) into a\n",
    "    UTC timestamp. Assumes the date is already in UTC.\n",
    "    \n",
    "    Args:\n",
    "        date_str: A string containing the RateMyProfessor review date.\n",
    "        \n",
    "    Returns:\n",
    "        A UTC timestamp corresponding to the date provided.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split into month, day, year\n",
    "    date_split = date_str.split(' ')\n",
    "    day = date_split[1]\n",
    "    \n",
    "    # Remove comma and suffix for day\n",
    "    day = day[:-3]\n",
    "    \n",
    "    # Place the day back into the list and join everything back together\n",
    "    date_split[1] = day\n",
    "    remade_date_str = (' ').join(date_split)\n",
    "    \n",
    "    # Change into UTC time\n",
    "    datetime_obj = datetime.datetime.strptime(remade_date_str, '%b %d %Y')\n",
    "    utc_time = datetime_obj.timestamp()\n",
    "    \n",
    "    return utc_time\n",
    "    \n",
    "def parse_rating_header(soup):\n",
    "    \"\"\"Parses and returns the rating header for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the course and date for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_header = soup.select('div[class*=\"Rating__RatingInfo\"]')\n",
    "    \n",
    "    if len(rating_header) != 0:\n",
    "        course = rating_header[0].select('div[class*=\"RatingHeader__StyledClass\"]')[0].text.strip()\n",
    "        date = rating_header[0].select('div[class*=\"TimeStamp__StyledTimeStamp\"]')[0].text.strip()\n",
    "        \n",
    "        utc_time = string_date_to_unix(date)\n",
    "    else:\n",
    "        print(soup)\n",
    "    \n",
    "    return {'course': course, 'date': utc_time}\n",
    "\n",
    "def parse_meta_data(soup):\n",
    "    \"\"\"Parses and returns the meta data for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the meta data (e.g. Would Take Again) for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    course_meta = soup.select('div[class*=\"CourseMeta__StyledCourseMeta\"]')[0]\n",
    "    review_meta_data = {}\n",
    "\n",
    "    for meta_div in course_meta.select('div'):\n",
    "        meta_data = meta_div.text.split(':')\n",
    "        meta_name = meta_data[0].strip()\n",
    "        meta_value = meta_data[1].strip()\n",
    "\n",
    "        review_meta_data[meta_name] = meta_value\n",
    "\n",
    "    return meta_to_dict(review_meta_data)\n",
    "\n",
    "def parse_rating_data(soup):\n",
    "    \"\"\"Parses and returns the rating data for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the rating data for the quality and difficulty for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    rating_values_text = soup.select('div[class*=\"RatingValues__StyledRatingValues\"]')[0].select('div[class*=\"RatingValues__RatingValue\"]')\n",
    "    quality = rating_values_text[0].text\n",
    "    difficulty = rating_values_text[1].text\n",
    "\n",
    "    rating_data = {'quality': quality, 'difficulty': difficulty}\n",
    "    \n",
    "    return rating_data\n",
    "\n",
    "def parse_review_tags(soup):\n",
    "    \"\"\"Parses and returns the tags for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing the tags for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_container = soup.select('div[class*=\"RatingTags__StyledTags\"]')\n",
    "    tags = []\n",
    "    \n",
    "    if tag_container: # Since not all reviews add tags\n",
    "        unparsed_tags = tag_container[0].select('span')\n",
    "\n",
    "        for tag in unparsed_tags:\n",
    "            tags.append(tag.text)\n",
    "\n",
    "    return tags_to_dict(tags)\n",
    "    \n",
    "def parse_thumb_scoring(soup):\n",
    "    \"\"\"Parses and returns the thumb scoring data for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the thumb scoring data for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    thumb_container = soup.select('div[class*=\"RatingFooter__StyledRatingFooter\"]')[0].select('div[class*=\"RatingFooter__HelpTotal\"]')\n",
    "\n",
    "    thumb_up = int(thumb_container[0].text.strip())\n",
    "    thumb_down = int(thumb_container[1].text.strip())\n",
    "    thumb_data = {'thumb_up': thumb_up, 'thumb_down': thumb_down}\n",
    "\n",
    "    return thumb_data\n",
    "\n",
    "def parse_review_text(soup):\n",
    "    \"\"\"Parses and returns the review body text for a single review.\n",
    "    \n",
    "    Args:\n",
    "        soup: An initialized BeautifulSoup object for the professor's page.\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the review text for the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    review_text = soup.select('div[class*=\"Comments__StyledComments\"]')[0].text\n",
    "    \n",
    "    return {'body': review_text}\n",
    "    \n",
    "def parse_single_rmp_review(review_item, courses):\n",
    "    \"\"\"Parses and returns all data for a single review.\n",
    "    Namely it returns: Meta data, rating data, tags, thumb_scoring, and review text.\n",
    "    \n",
    "    Args:\n",
    "        review_item: A single review list item containing all the appropraite HTML.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the meta data, rating data, tags, thumb_scoring, and review text\n",
    "        for a single review.\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(review_item, 'html.parser')\n",
    "    \n",
    "    course_and_date = parse_rating_header(soup)\n",
    "    \n",
    "    # TODO: Loses course reviews like 'CMSC131CMSC132' where students combined multiple courses they took\n",
    "    if course_and_date['course'] in courses:\n",
    "        \n",
    "        # Meta data\n",
    "        meta_data = parse_meta_data(soup)\n",
    "        \n",
    "        # Rating data\n",
    "        rating_data = parse_rating_data(soup)\n",
    "        \n",
    "        # Tags \n",
    "        tags = parse_review_tags(soup)\n",
    "        \n",
    "        # Thumb Scoring\n",
    "        thumb_scoring = parse_thumb_scoring(soup)\n",
    "        \n",
    "        # Review body\n",
    "        review_text = parse_review_text(soup)\n",
    "        \n",
    "        return {'meta_data': meta_data, 'rating_data': rating_data, 'tags': tags, 'thumb_scoring': thumb_scoring,\n",
    "                'review_text': review_text, 'rating_header': course_and_date}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection 2.6: Parsing Utilities for an Entire RateMyProfessor Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmp_prof_reviews(rmp_prof_url, selenium_driver, prof_name, courses):\n",
    "    \"\"\"Gets all the RateMyProfessor reviews for a given professor and places into a\n",
    "    dataframe. Only grabs reviews for classes in the provided courses.\n",
    "    \n",
    "    Args:\n",
    "        rmp_prof_url: A string containing the RateMyProfessor URL for the professor.\n",
    "        prof_name: A string containing the professor's name.\n",
    "        prof_courses: List of courses to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe containing all the appropriate reviews.\n",
    "    \"\"\"\n",
    "    \n",
    "    reviews_html = load_all_rmp_reviews(rmp_prof_url, selenium_driver)\n",
    "    soup = BeautifulSoup(reviews_html, 'html.parser')\n",
    "    \n",
    "    first_name, last_name = prof_name.split(' ', 1)\n",
    "    review_id_head = prof_name + '-'\n",
    "    counter = 1\n",
    "    \n",
    "    review_df = pd.DataFrame(columns=review_header_list)\n",
    "    \n",
    "    for review in soup.find_all('li'):\n",
    "        \n",
    "        if len(review.select('div[class*=\"Rating__StyledRating\"]')) != 0: # Avoid advertisement list items\n",
    "            data = parse_single_rmp_review(str(review), courses)\n",
    "\n",
    "            cur_review_id = review_id_head + str(counter)\n",
    "            counter = counter + 1\n",
    "            \n",
    "            if data: # Since the review could be of an undesired course\n",
    "                flattened_data = {'first_name': first_name, 'last_name': last_name, 'full_name': prof_name,\n",
    "                                  'review_id': cur_review_id}\n",
    "\n",
    "                for data_type, data_dict in data.items():\n",
    "                    \n",
    "                    for key, val in data_dict.items():\n",
    "                        flattened_data[key] = val\n",
    "\n",
    "                review_df = review_df.append(flattened_data, ignore_index=True)\n",
    "    \n",
    "    return review_df\n",
    "\n",
    "\n",
    "def parse_rmp_page(rmp_prof_url, headers, rmp_conn, selenium_driver, professor_name, courses):\n",
    "    \"\"\"Parses an entire RateMyProfessor professor page for overall stats & tags, and all\n",
    "    of their reviews. It will return two dataframes holding this information and insert\n",
    "    them into a database.\n",
    "    \n",
    "    Args:\n",
    "        rmp_prof_url: A string containing the RateMyProfessor URL for the professor.\n",
    "        headers: Request headers to use.\n",
    "        rmp_conn: Connection object to the RateMyProfessor database.\n",
    "        prof_name: A string containing the professor's name.\n",
    "        courses: List of courses to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of two dataframes, (overall statistics, all the reviews).\n",
    "    \"\"\"\n",
    "    \n",
    "    rmp_prof_page = requests.get(rmp_prof_url, headers=headers)\n",
    "    \n",
    "    if rmp_prof_page.status_code == 200:\n",
    "        soup = BeautifulSoup(rmp_prof_page.text, 'html.parser')\n",
    "        \n",
    "        # Professor stats\n",
    "        stats_container = soup.select('div[class*=\"TeacherInfo__StyledTeacher\"]')[0]\n",
    "        \n",
    "        prof_stats = get_rmp_prof_stats(str(stats_container))\n",
    "        prof_tags = get_rmp_prof_top_tags(str(stats_container))\n",
    "        \n",
    "        overall_df = rmp_prof_overall_to_dataframe(professor_name, prof_stats, prof_tags)\n",
    "        insert_dataframe_into_db(rmp_conn, overall_df, 'professor_stats')\n",
    "        \n",
    "        # Professor reviews\n",
    "        all_reviews_df = get_rmp_prof_reviews(rmp_prof_url, selenium_driver, professor_name, courses)\n",
    "        insert_dataframe_into_db(rmp_conn, all_reviews_df, 'reviews')\n",
    "        \n",
    "        return (overall_df, all_reviews_df)\n",
    "    else:\n",
    "        print(\"Error opening the RateMyProfessor professor page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection 2.7: Scrape and Parse All Professors Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nonexistant_rmp_data(rmp_conn, professor):\n",
    "    \"\"\"Marks a professor as not having a page and fills professor's overall statistics dataframe\n",
    "    with empty values so that it may be placed into the database and not re-queried for later.\n",
    "    \n",
    "    Args:\n",
    "        rmp_db: Connection object to the RateMyProfessor database.\n",
    "        professor: String containing the name of the professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    empty_tags = tags_to_dict([])\n",
    "    empty_stats = {'rating': 0, 'take_again': 0, 'difficulty': 0, 'rating_count': 0}\n",
    "    \n",
    "    # Professor stats\n",
    "    overall_df = rmp_prof_overall_to_dataframe(professor, empty_stats, empty_tags, page_exists=0)\n",
    "    insert_dataframe_into_db(rmp_conn, overall_df, 'professor_stats')\n",
    "    \n",
    "    return None\n",
    "    \n",
    "    \n",
    "def parse_rmp_all_professors(rmp_db_filepath, professors, provided_courses, force_scrape=False):\n",
    "    \"\"\"Scrapes and parses all professors, storing the data in a database and returning a\n",
    "    list of dataframes for stats and reviews.\n",
    "    \n",
    "    Args:\n",
    "        rmp_db_filepath: String containing the filepath to the appropriate database.\n",
    "        professors: Dictionary of professors to list of courses.\n",
    "        force_scrape (optional, default=False): Forces a scrape of RateMyProfessors even if already done.\n",
    "        \n",
    "    Returns:\n",
    "        The tuple (stats, reviews) where each is a list of dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_major_stats = []\n",
    "    all_major_reviews = []\n",
    "    \n",
    "    rmp_db = create_connection(rmp_db_filepath)\n",
    "    selenium_driver = start_selenium()\n",
    "    \n",
    "    try:\n",
    "        for professor, courses in professors.items():\n",
    "            overall_stats_df = None\n",
    "            all_reviews_df = None\n",
    "\n",
    "            # Read from database if the professor has already been scraped (only checks stats for confirmation)\n",
    "            if not force_scrape and is_professor_scraped(rmp_db, professor):\n",
    "                overall_stats_df = get_professor_stats_from_db(rmp_db, professor)\n",
    "                all_reviews_df = get_professor_reviews_from_db(rmp_db, professor)\n",
    "\n",
    "                # Keep track of the dataframes for each professor\n",
    "                all_major_stats.append(overall_stats_df)\n",
    "                all_major_reviews.append(all_reviews_df)\n",
    "\n",
    "            else:\n",
    "                # Get all the data from the professor's RateMyProfessor page\n",
    "                prof_rmp_url = query_rmp_for_professor_url(professor, headers, params)\n",
    "\n",
    "                # If the professor has a RateMyProfessor page\n",
    "                if prof_rmp_url is not None:\n",
    "                    overall_stats_df, all_reviews_df = parse_rmp_page(prof_rmp_url, headers, rmp_db, selenium_driver, professor, provided_courses)\n",
    "\n",
    "                    # Keep track of the dataframes for each professor\n",
    "                    all_major_stats.append(overall_stats_df)\n",
    "                    all_major_reviews.append(all_reviews_df)\n",
    "                    \n",
    "                    # So we don't query RateMyProfessor too much\n",
    "                    time.sleep(1)\n",
    "\n",
    "                else:\n",
    "                    # Used to fill the stats table to show their page doesn't exist\n",
    "                    fill_nonexistant_rmp_data(rmp_db, professor)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        if should_store_data:\n",
    "            rmp_db.close()\n",
    "            \n",
    "        stop_selenium(selenium_driver)\n",
    "    \n",
    "        return (all_major_stats, all_major_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.8: Scrape and Parse All Computer Science Professors from RateMyProfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmp_cmsc_stats, all_rmp_cmsc_reviews = parse_rmp_all_professors(cmsc_rmp_db_filepath, cmsc_professors, cmsc_course_ids)\n",
    "\n",
    "merged_rmp_cmsc_stats = pd.concat(all_rmp_cmsc_stats)\n",
    "merged_rmp_cmsc_reviews = pd.concat(all_rmp_cmsc_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_cmsc_stats))\n",
    "merged_rmp_cmsc_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_cmsc_reviews))\n",
    "merged_rmp_cmsc_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 2.9: Scrape and Parse All Business Management Professors from RateMyProfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmp_bmgt_stats, all_rmp_bmgt_reviews = parse_rmp_all_professors(bmgt_rmp_db_filepath, bmgt_professors, bmgt_course_ids)\n",
    "\n",
    "merged_rmp_bmgt_stats = pd.concat(all_rmp_bmgt_stats)\n",
    "merged_rmp_bmgt_reviews = pd.concat(all_rmp_bmgt_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_bmgt_stats))\n",
    "merged_rmp_bmgt_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_rmp_bmgt_reviews))\n",
    "merged_rmp_bmgt_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 3: Query and Parse Data from PlanetTerp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.1: Setup and Utilities for PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://api.planetterp.com/#get-a-professor\n",
    "planetterp_api = \"https://api.planetterp.com/v1/professor\"\n",
    "pt_header = {'Accept': 'application/json'}\n",
    "params = {'reviews': 'true'}\n",
    "\n",
    "# Stats and reviews data\n",
    "stats_columns=['first_name', 'last_name', 'full_name', 'slug', 'review_count', 'type', 'page_exists']\n",
    "review_columns=['review_id', 'full_name', 'course', 'date', 'body', 'rating', 'expected_grade']\n",
    "\n",
    "# Grade data\n",
    "grades_list = ['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-',\n",
    "                  'D+', 'D', 'D-', 'F', 'W']\n",
    "grades_db_list = ['a_plus', 'a', 'a_minus', 'b_plus', 'b', 'b_minus','c_plus', 'c', 'c_minus','d_plus', 'd', 'd_minus', 'f', 'w']\n",
    "\n",
    "grades_headers = ['course', 'semester'] + grades_list\n",
    "grades_db_headers = ['course', 'semester'] + grades_db_list \n",
    "\n",
    "grades_header_dict = {grades_headers[i]: grades_db_headers[i] for i in range(len(grades_headers))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_date_to_unix(date_str):\n",
    "    \"\"\"Takes the PlanetTerp datetime string and converts to unix time. Assumes\n",
    "    It is already in UTC timezone.\n",
    "    \n",
    "    Args:\n",
    "        date_str: String containing a date time in the format \"%Y-%m-%dT%H:%M:%S\".\n",
    "        \n",
    "    Returns:\n",
    "        A unix timestamp real representing the time passed into the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format: 2020-01-01T00:00:00\n",
    "    date_time_obj = datetime.datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    return date_time_obj.timestamp() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.2: Parsing PlanetTerp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Could simply use the review JSON provided, but may not be the format we want\n",
    "def parse_pt_single_review(review, review_id, courses):\n",
    "    \"\"\"Parses a single PlanetTerp review and places it into a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        review: A dictionary or JSON object holding the review data.\n",
    "        review_id: A string holding an unique id for this review.\n",
    "        courses: List of course ids to determine if review wanted.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary holding review information.\n",
    "    \"\"\"\n",
    "    \n",
    "    review_dict = {}\n",
    "    course = review.get('course')\n",
    "    \n",
    "    if course and course in courses:\n",
    "        review_dict = {'full_name': review.get('professor'), 'course': course,\n",
    "                       'body': review.get('review'), 'expected_grade': review.get('expected_grade', np.nan),\n",
    "                       'rating': review.get('rating')}\n",
    "\n",
    "        unix_time = pt_date_to_unix(review.get('created'))\n",
    "        review_dict['date'] = unix_time\n",
    "\n",
    "        review_dict['review_id'] = review_id\n",
    "        \n",
    "    return review_dict\n",
    "    \n",
    "    \n",
    "def parse_pt_reviews(reviews, courses):\n",
    "    \"\"\"Parses all reviews from PlanetTerp, placing those that are within the desired courses\n",
    "    into a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        reviews: A list of dictionaries, each dictionary representing a single review.\n",
    "        reviews_df: A dataframe to hold the reviews.\n",
    "        courses: The desired courses for which to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe containing all the reviews for a professor.\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_rating = 0\n",
    "    course_count = 0\n",
    "    \n",
    "    reviews_df = pd.DataFrame(columns=review_columns)\n",
    "    idx = 1\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_id = review.get('professor') + '-' + str(idx)\n",
    "        idx = idx + 1\n",
    "        \n",
    "        review_dict = parse_pt_single_review(review, review_id, courses)\n",
    "        \n",
    "        if bool(review_dict):\n",
    "            reviews_df = reviews_df.append(review_dict, ignore_index=True)\n",
    "            avg_rating = avg_rating + review_dict['rating']\n",
    "            course_count = course_count + 1\n",
    "            \n",
    "    if course_count != 0:\n",
    "        avg_rating = float(avg_rating) / course_count\n",
    "        \n",
    "    return (reviews_df, avg_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.3: Querying PlanetTerp for Professors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pt_for_professor(professor, courses):\n",
    "    \"\"\"Queries the PlanetTerp API for a given professor, gathering their stats\n",
    "    and reviews. It then returns two dataframes (stats, reviews).\n",
    "    \n",
    "    Args:\n",
    "        professor: String holding the name of the professor to query.\n",
    "        courses: List of course ids to look for in the reviews.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (stats, reviews) of dataframes holding the stats and reviews data.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats_df = pd.DataFrame(columns=stats_columns)\n",
    "    reviews_df = pd.DataFrame()\n",
    "    \n",
    "    params['name'] = professor\n",
    "    \n",
    "    \n",
    "    response = requests.get(planetterp_api, headers=pt_header, params=params)\n",
    "    \n",
    "    first_name, last_name = professor.split(' ', 1)\n",
    "    prof_stats = {'first_name': first_name, 'last_name': last_name, \n",
    "                  'full_name': professor}\n",
    "    \n",
    "    # The professor may not exist in the PlanetTerp database (though this shouldn't occur)\n",
    "    if response.status_code == 200:\n",
    "        json = response.json()\n",
    "        \n",
    "        review_count = 0\n",
    "        avg_rating = None\n",
    "        reviews = json.get('reviews')\n",
    "        \n",
    "        # The professor may not have any reviews\n",
    "        if reviews:\n",
    "            review_count = len(reviews)\n",
    "            reviews_df, avg_rating = parse_pt_reviews(reviews, courses)\n",
    "            \n",
    "        stats_cont = {'slug': json.get('slug'), 'type': json.get('type'),\n",
    "                     'review_count': review_count, 'rating': avg_rating, 'page_exists': 1}\n",
    "        \n",
    "    else:\n",
    "        stats_cont = {'page_exists': 0, 'review_count': 0}\n",
    "        \n",
    "        \n",
    "    prof_stats.update(stats_cont)\n",
    "    stats_df = stats_df.append(prof_stats, ignore_index=True)\n",
    "    \n",
    "    return (stats_df, reviews_df)\n",
    "\n",
    "\n",
    "def query_pt_for_all_professors(professors, courses, db_filepath, force_query=False):\n",
    "    \"\"\"Queries PlanetTerp for all the professors provided, taking reviews that\n",
    "    correspond to the given courses, and places professor stats and \n",
    "    reviews into a database.\n",
    "    \n",
    "    Args:\n",
    "        professors: A list of strings containing professor names.\n",
    "        courses: A list of strings containing course ids.\n",
    "        db_filepath: A string holding the filepath to a database.\n",
    "        force_query (optional, default=False): Boolean to decied whether to force\n",
    "            query the PlanetTerp API.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (stats, reviews) of lists containing all dataframes for\n",
    "        each professor stats and reviews respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_major_stats = []\n",
    "    all_major_reviews = []\n",
    "    \n",
    "    try:\n",
    "        pt_db = create_connection(db_filepath)\n",
    "        for professor in professors:\n",
    "            \n",
    "            if not force_query and is_professor_scraped(pt_db, professor):\n",
    "                \n",
    "                stats_df = get_professor_stats_from_db(pt_db, professor)\n",
    "                reviews_df = get_professor_reviews_from_db(pt_db, professor)\n",
    "\n",
    "                # Keep track of the dataframes for each professor\n",
    "                all_major_stats.append(stats_df)\n",
    "                all_major_reviews.append(reviews_df)\n",
    "                \n",
    "            else:\n",
    "                stats_df, reviews_df = query_pt_for_professor(professor, courses)\n",
    "\n",
    "                if not stats_df.empty:\n",
    "                    all_major_stats.append(stats_df)\n",
    "                    insert_dataframe_into_db(pt_db, stats_df, 'professor_stats')\n",
    "\n",
    "                if not reviews_df.empty:\n",
    "                    all_major_reviews.append(reviews_df)\n",
    "                    insert_dataframe_into_db(pt_db, reviews_df, 'reviews')\n",
    "\n",
    "                time.sleep(1) # To give some time to the PlanetTerp API\n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        if should_store_data:\n",
    "            pt_db.close()\n",
    "            \n",
    "        return (all_major_stats, all_major_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_semester_grades(course_df):\n",
    "    \"\"\"Sums up grades across the same course and semester and returns a\n",
    "    new dataframe containing this information. Course grades are originally\n",
    "    grouped by their section and professor, so we want to aggregate them.\n",
    "    \n",
    "    Args:\n",
    "        course_df: A dataframe holding grades per section and professor.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe where the identical courses and semesters have their\n",
    "        grades aggregated.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=course_df.columns)\n",
    "    semester_groups = course_df.groupby(['course', 'semester']).sum().reset_index()\n",
    "    return semester_groups\n",
    "\n",
    "def accumulate_course_grades(course_grades_json):\n",
    "    \"\"\"Accumulates all the grades of a course into a dataframe, because course\n",
    "    grades will be separated out by their section and professor.\n",
    "    \n",
    "    Args:\n",
    "        course_grades_json: A dictionary from PlanetTerp API containing grade info\"\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe where all grades of the same course and semester are accumulated.\n",
    "    \"\"\"\n",
    "    \n",
    "    course_grades_dict =  {}\n",
    "    course_grades_df = pd.DataFrame(columns=grades_header_dict.values())\n",
    "    \n",
    "    for course in course_grades_json:\n",
    "        course.pop('professor', None)\n",
    "        course.pop('Other', None)\n",
    "        course.pop('section', None)\n",
    "        \n",
    "        for human_header, db_header in grades_header_dict.items():\n",
    "            course_grades_dict[db_header] = course.pop(human_header)\n",
    "        \n",
    "        course_grades_df = course_grades_df.append(course_grades_dict, ignore_index=True)\n",
    "        \n",
    "    return sum_semester_grades(course_grades_df)\n",
    "    \n",
    "def query_pt_for_course_grades(courses, db_filepath, force_query=False):\n",
    "    \"\"\"Queries PlanetTerp for the grades for each course, accumulates\n",
    "    all grades of identitical courses and semesters, and places into a database.\n",
    "    \n",
    "    Args:\n",
    "        courses: A list of course ids to query the grades PlantTerp for\n",
    "        db_filepath: A database filepath to open and insert data into\n",
    "        force_query (optional, default=False): Boolean determining whether\n",
    "            PlanetTerp should be queried, regardless of database info.\n",
    "            \n",
    "    Returns:\n",
    "        A dataframe of all course, semester grades accumulated.\n",
    "    \"\"\"\n",
    "    \n",
    "    pt_db = create_connection(db_filepath)\n",
    "    \n",
    "    grades_api = 'https://api.planetterp.com/v1/grades'\n",
    "    grades_params = {'course': None}\n",
    "    \n",
    "    all_course_grades = []\n",
    "    \n",
    "    try:\n",
    "        for course in courses:\n",
    "            \n",
    "            # Check whether we've already queried for this course\n",
    "            course_grades_df = get_course_grades_from_db(pt_db, course)\n",
    "            \n",
    "            if course_grades_df.empty:\n",
    "                course_grades_df = pd.DataFrame(columns=grades_header_dict.values())\n",
    "            \n",
    "            if force_query or course_grades_df.empty:\n",
    "                grades_params['course'] = course\n",
    "                response = requests.get(grades_api, headers=pt_header, params=grades_params)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    grade_data = response.json()\n",
    "                    \n",
    "                    # Accumulate the grade info for this course\n",
    "                    course_grades_df = accumulate_course_grades(grade_data)\n",
    "                    all_course_grades.append(course_grades_df)\n",
    "\n",
    "                    # Put the course grades into the database\n",
    "                    insert_dataframe_into_db(pt_db, course_grades_df, 'grades', column_headers=grades_header_dict.values())\n",
    "                    \n",
    "                    # Give the API a bit of time\n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "            else:\n",
    "                all_course_grades.append(course_grades_df)\n",
    "                \n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Type error: \" + str(e))\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        \n",
    "        if should_store_data:\n",
    "            pt_db.close()\n",
    "        \n",
    "        if len(all_course_grades) == 0:\n",
    "            print(\"Error getting any course grades\")\n",
    "            return None\n",
    "        \n",
    "        return pd.concat(all_course_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.4: Parse All Computer Science Professors from PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_cmsc_stats, all_pt_cmsc_reviews = query_pt_for_all_professors(cmsc_professors, cmsc_course_ids, cmsc_pt_db_filepath)\n",
    "all_pt_cmsc_grades = query_pt_for_course_grades(cmsc_course_ids, cmsc_pt_db_filepath)\n",
    "\n",
    "merged_pt_cmsc_stats = pd.concat(all_pt_cmsc_stats)\n",
    "merged_pt_cmsc_reviews = pd.concat(all_pt_cmsc_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_cmsc_stats))\n",
    "merged_pt_cmsc_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_cmsc_reviews))\n",
    "merged_pt_cmsc_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_cmsc_grades))\n",
    "all_pt_cmsc_grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Part 3.5: Parse All Business Management Professors from PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pt_bmgt_stats, all_pt_bmgt_reviews = query_pt_for_all_professors(bmgt_professors, bmgt_course_ids, bmgt_pt_db_filepath)\n",
    "all_pt_bmgt_grades = query_pt_for_course_grades(bmgt_course_ids, bmgt_pt_db_filepath)\n",
    "\n",
    "merged_pt_bmgt_stats = pd.concat(all_pt_bmgt_stats)\n",
    "merged_pt_bmgt_reviews = pd.concat(all_pt_bmgt_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_bmgt_stats))\n",
    "merged_pt_bmgt_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_pt_bmgt_reviews))\n",
    "merged_pt_bmgt_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_bmgt_grades))\n",
    "all_pt_bmgt_grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Part 4: Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Across PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that we can separate by major later\n",
    "merged_pt_bmgt_stats['major'] = 'bmgt'\n",
    "merged_pt_cmsc_stats['major'] = 'cmsc'\n",
    "\n",
    "merged_pt_bmgt_reviews['major'] = 'bmgt'\n",
    "merged_pt_cmsc_reviews['major'] = 'cmsc'\n",
    "\n",
    "all_pt_stats = merged_pt_bmgt_stats.append(merged_pt_cmsc_stats)\n",
    "all_pt_reviews = merged_pt_bmgt_reviews.append(merged_pt_cmsc_reviews)\n",
    "all_pt_grades = pd.concat([all_pt_cmsc_grades, all_pt_bmgt_grades])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_stats))\n",
    "all_pt_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_reviews))\n",
    "all_pt_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pt_grades))\n",
    "all_pt_grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Across RateMyProfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that we can separate by major later\n",
    "merged_rmp_bmgt_stats['major'] = 'bmgt'\n",
    "merged_rmp_cmsc_stats['major'] = 'cmsc'\n",
    "\n",
    "merged_rmp_bmgt_reviews['major'] = 'bmgt'\n",
    "merged_rmp_cmsc_reviews['major'] = 'cmsc'\n",
    "\n",
    "all_rmp_stats = merged_rmp_bmgt_stats.append(merged_rmp_cmsc_stats)\n",
    "all_rmp_reviews = merged_rmp_bmgt_reviews.append(merged_rmp_cmsc_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_rmp_stats))\n",
    "all_rmp_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_rmp_reviews))\n",
    "all_rmp_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Across both RateMyProfessor and PlanetTerp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_stats_cols = ['full_name', 'page_exists', 'rating', 'major']\n",
    "all_stats = pd.concat([all_pt_stats[shared_stats_cols + ['review_count']], all_rmp_stats[shared_stats_cols + ['rating_count']]])\n",
    "\n",
    "# Combine the rating counts into a single column and sum the total\n",
    "all_stats.replace(np.nan, 0, inplace=True)\n",
    "all_stats['total_reviews'] = all_stats['review_count'] + all_stats['rating_count']\n",
    "all_stats.drop(['review_count', 'rating_count'], inplace=True, axis=1)\n",
    "\n",
    "# Average the rating given to the professor across both RMP and PT\n",
    "all_stats = all_stats.groupby(['full_name', 'major']).agg({'rating': 'mean', 'total_reviews': 'sum'}).reset_index()\n",
    "all_stats = all_stats[all_stats.total_reviews != 0]\n",
    "all_stats.rename(columns={'rating': 'avg_rating'}, inplace=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_stats))\n",
    "all_stats.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_review_cols = ['full_name', 'course', 'date', 'body', 'major']\n",
    "all_reviews = pd.concat([all_pt_reviews[shared_review_cols + ['expected_grade']], all_rmp_reviews[shared_review_cols + ['grade']]])\n",
    "\n",
    "# Combine the grades into a single column\n",
    "def remove_bad_grade_values(val):\n",
    "    if val not in grades_list:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "# TODO: Maybe could keep the non-grade values and be used another way?\n",
    "all_reviews['grade'] = all_reviews['grade'].map(remove_bad_grade_values)\n",
    "all_reviews['expected_grade'].replace(np.nan, '', inplace=True)\n",
    "all_reviews['grade'].replace(np.nan, '', inplace=True)\n",
    "\n",
    "all_reviews['reviewer_grade'] = all_reviews['expected_grade'] + all_reviews['grade']\n",
    "\n",
    "all_reviews.drop(['expected_grade', 'grade'], axis=1, inplace=True)\n",
    "print(all_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_reviews))\n",
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Put together majors as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cmsc_stats = all_stats[all_stats['major'] == 'cmsc']\n",
    "all_cmsc_reviews = all_reviews[all_reviews['major'] == 'cmsc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_cmsc_stats))\n",
    "all_cmsc_stats.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_cmsc_reviews))\n",
    "all_cmsc_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bmgt_stats = all_stats[all_stats['major'] == 'bmgt']\n",
    "all_bmgt_reviews = all_reviews[all_reviews['major'] == 'bmgt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_bmgt_stats))\n",
    "all_bmgt_stats.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_bmgt_reviews))\n",
    "all_bmgt_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Breadth of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(columns=['major', 'count'])\n",
    "stats_df['major'] = ['cmsc', 'bmgt']\n",
    "stats_df['count'] = [len(all_cmsc_stats), len(all_bmgt_stats)]\n",
    "\n",
    "ax = sns.barplot(x='major', y='count', data=stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(columns=['major', 'count'])\n",
    "reviews_df['major'] = ['cmsc', 'bmgt']\n",
    "reviews_df['count'] = [len(all_cmsc_reviews), len(all_bmgt_reviews)]\n",
    "\n",
    "ax = sns.barplot(x='major', y='count', data=reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_df_header = ['major'] + grades_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Word Frequencies and Rudimentary Sentiment Analysis in Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "non_lemma_words = [\"cs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    cleaned_text = re.sub(r'[\\W]+', ' ', text)\n",
    "    word_list = cleaned_text.split(' ')\n",
    "    \n",
    "    return word_list\n",
    "    \n",
    "    \n",
    "def lemmatize_word_list(word_list):\n",
    "    cleaned_list = [word.lower() for word in word_list if word not in default_stopwords]\n",
    "    lemma_list = [wnl.lemmatize(word) if word not in non_lemma_words else word for word in cleaned_list]\n",
    "    \n",
    "    return lemma_list \n",
    "\n",
    "\n",
    "def create_word_freq(text):\n",
    "    lowered = text.lower()\n",
    "    tagged_token_list = pos_tag(word_tokenize(lowered))\n",
    "    \n",
    "    for word, tag in tagged_token_list:\n",
    "        l_tag = tag.lower()\n",
    "        \n",
    "    \n",
    "    freqDist = nltk.FreqDist(lemma_list)\n",
    "    \n",
    "    return freqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_word_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-639095a492e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Testing...the, FeAtUrE123 < !! @@ ## Working. Was. Is. Be. Bad badly worst great good awesome terrible\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfreqDist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_word_freq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfreqDist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_word_freq' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Testing...the, FeAtUrE123 < !! @@ ## Working. Was. Is. Be. Bad badly worst great good awesome terrible\"\n",
    "freqDist = create_word_freq(text)\n",
    "\n",
    "for lemma, freq in freqDist.most_common(10):\n",
    "    print(\"{}: {}\".format(lemma, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SpaCy instead, seems to be cleaner and bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Need to run python -m spacy download en_core_web_sm (also a medium and large dataset)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# Credit to https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "def tokenize_lemmaize_text(text):\n",
    "    tokens = nlp(text)\n",
    "    \n",
    "    tokens = [word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word in tokens]\n",
    "    tokens = [lemma for lemma in tokens if lemma not in spacy_stopwords and lemma not in punctuation]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teli be a great professor . -PRON- would take -PRON- again .\n"
     ]
    }
   ],
   "source": [
    "text = \"Teli is a great professor. I would take him again.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(' '.join([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['teli', 'great', 'professor']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_lemmaize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Review Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word Associations and Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Review Trends and Trends Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
